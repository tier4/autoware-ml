# @package _global_

callbacks:
  model_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: val/loss
    dirpath: ${hydra:run.dir}/checkpoints
    filename: best
    save_top_k: 1
    save_last: true
    mode: min
    verbose: true

  early_stopping:
    _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: val/loss
    patience: 20
    mode: min

  lr_monitor:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: step
